{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Usage: python script.py <data_file>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine  # to save the clean dataset into an sqlite database\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle  # for ML-model export as a pickle file\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('wordnet') # lexical database of English\n",
    "nltk.download('punkt') # tokenizer model split text into words\n",
    "nltk.download('stopwords') # list of common stopword to remove them\n",
    "\n",
    "# Tokenization function \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Machine learning pipeline\n",
    "from sklearn.pipeline import Pipeline  # For creating the pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  # For text processing\n",
    "from sklearn.multioutput import MultiOutputClassifier  # For multi-output classification\n",
    "from sklearn.ensemble import RandomForestClassifier  # For the Random Forest classifier\n",
    "\n",
    "# Training of pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Test training model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Improve model with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV  # for using GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \"\"\"\n",
    "    Loads and cleans the messages and categories datasets.\n",
    "\n",
    "    Parameters:\n",
    "    messages_filepath (str): The file path to the CSV file containing the messages.\n",
    "    categories_filepath (str): The file path to the CSV file containing the categories.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple consisting of:\n",
    "        - X (pd.Series): The messages (feature).\n",
    "        - y (pd.DataFrame): The categories (labels), split into separate columns.\n",
    "    \"\"\"\n",
    "       \n",
    "    # read in data files (two csv files)\n",
    "    messages = pd.read_csv(messages_filepath) # load messages dataset\n",
    "    categories = pd.read_csv(categories_filepath) # load categories dataset\n",
    "    df = pd.merge(messages, categories, left_on='id', right_on='id', how='inner')  # merge datasets\n",
    "\n",
    "    # clean data\n",
    "    categories = df[\"categories\"].str.split(';', expand=True) # Split the 'categories' column into separate columns\n",
    "    categories.columns = categories.iloc[0].str[:-2].values # Extract new column names directly from the first row\n",
    "    categories = categories[1:] # Drop the first row since it was used for column names\n",
    "\n",
    "    for column in categories:\n",
    "        categories[column] = categories[column].str[-1] # set each value to be the last character of the string\n",
    "        categories[column] = pd.to_numeric(categories[column]) # convert column from string to numeric\n",
    "\n",
    "    df.drop(['categories'], axis=1, inplace = True) # drop the original categories column from `df`\n",
    "    df = pd.concat([df, categories], axis=1) # concatenate the original dataframe with the new `categories` dataframe\n",
    "\n",
    "    df = df.drop_duplicates() # Remove duplicates\n",
    "    df.fillna(0, inplace=True) # Replace all NaN values with 0\n",
    "\n",
    "    # load to database / Save the clean dataset into an sqlite database.\n",
    "    engine = create_engine('sqlite:///DisasterResponseProject.db')\n",
    "    df.to_sql('DisasterResponses', engine, index=False, if_exists='replace')\n",
    "\n",
    "    # define features and label arrays\n",
    "    X = df['message']\n",
    "    y = df.iloc[:,4:]\n",
    "    print(\"load data\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):    \n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes the input text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cleaned tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # text processing: tokenization function to process data\n",
    "\n",
    "    url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' # Define a regex pattern to detect URLs\n",
    "    text = re.sub(url_regex, \"urlplaceholder\", text) # Replace URLs with a placeholder\n",
    "    tokens = word_tokenize(text.lower()) # Normalize and tokenize text\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\") and w.isalpha()] # Remove stopwords\n",
    "        \n",
    "    lemmatizer = WordNetLemmatizer() # Initiate lemmatizer\n",
    "    clean_tokens = []\n",
    "    for tok in tokens: # # Iterate through each token\n",
    "        # Lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    print(\"tokenize\")\n",
    "    return clean_tokens    \n",
    "\n",
    "    \n",
    "    \n",
    "def build_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds a machine learning pipeline for multi-output classification.\n",
    "\n",
    "    This function creates a pipeline that processes text data using a \n",
    "    CountVectorizer to convert text into a matrix of token counts, \n",
    "    followed by a TfidfTransformer to transform the count matrix to \n",
    "    a normalized term-frequency or TF-IDF representation. Finally, \n",
    "    it applies a MultiOutputClassifier with a RandomForestClassifier \n",
    "    as the base estimator to handle multi-label classification tasks.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: A scikit-learn Pipeline object that encapsulates the \n",
    "        text processing and classification steps.\n",
    "    \"\"\"\n",
    "    # Build a machine learning pipeline\n",
    "    machine_learning_pipeline = Pipeline([\n",
    "        ('cvect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    print(\"build ml model\")\n",
    "    return machine_learning_pipeline\n",
    "\n",
    "\n",
    "def train(X, y, model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains the given machine learning model on the provided features and labels.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.Series): The input features for training.\n",
    "    y (pd.DataFrame): The target labels for training.\n",
    "    model: The machine learning model to be trained.\n",
    "\n",
    "    Returns:\n",
    "    model: The trained machine learning model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Specify test_size and random_state\n",
    "    model.fit(X_train, y_train)  # Fit the model to the training data\n",
    "    print(\"train ml model\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def export_model(model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Exports the trained machine learning model as a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    model: The trained machine learning model to be exported.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Export model as a pickle file\n",
    "    # pickle.dump(machine_learning_pipeline_optimized, open('model.pkl', 'wb'))\n",
    "\n",
    "    # Export model as a pickle file\n",
    "    with open('model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)  # Use the model parameter instead of an undefined variable\n",
    "    print(\"export model\")\n",
    "    \n",
    "\n",
    "\n",
    "def run_pipeline(data_file):\n",
    "    X, y = load_data(data_file)  # run ETL pipeline\n",
    "    model = build_model()  # build model pipeline\n",
    "    model = train(X, y, model)  # train model pipeline\n",
    "    export_model(model)  # save model\n",
    "    print(\"run pipeline build, train, export model\")\n",
    "    \n",
    "#original\n",
    "#if __name__ == '__main__':\n",
    "#    data_file = sys.argv[1]  # get filename of dataset\n",
    "#    run_pipeline(data_file)  # run data pipeline\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python script.py <data_file>\")\n",
    "        sys.exit(1)  # Exit if the argument is not provided\n",
    "\n",
    "    data_file = sys.argv[1]  # get filename of dataset\n",
    "    run_pipeline(data_file)  # run data pipeline    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame (df):\n",
      "0    Weather update - a cold front from Cuba that c...\n",
      "1              Is the Hurricane over or is it not over\n",
      "2                      Looking for someone but no name\n",
      "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
      "4    says: west side of Haiti, rest of the country ...\n",
      "Name: message, dtype: object\n",
      "\n",
      "Features (X):\n",
      "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
      "0      NaN      NaN    NaN          NaN           NaN               NaN   \n",
      "1      1.0      0.0    0.0          1.0           0.0               0.0   \n",
      "2      1.0      0.0    0.0          0.0           0.0               0.0   \n",
      "3      1.0      1.0    0.0          1.0           0.0               1.0   \n",
      "4      1.0      0.0    0.0          0.0           0.0               0.0   \n",
      "\n",
      "   search_and_rescue  security  military  child_alone      ...        \\\n",
      "0                NaN       NaN       NaN          NaN      ...         \n",
      "1                0.0       0.0       0.0          0.0      ...         \n",
      "2                0.0       0.0       0.0          0.0      ...         \n",
      "3                0.0       0.0       0.0          0.0      ...         \n",
      "4                0.0       0.0       0.0          0.0      ...         \n",
      "\n",
      "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
      "0          NaN                   NaN              NaN     NaN    NaN   NaN   \n",
      "1          0.0                   0.0              1.0     0.0    1.0   0.0   \n",
      "2          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "3          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "4          0.0                   0.0              0.0     0.0    0.0   0.0   \n",
      "\n",
      "   earthquake  cold  other_weather  direct_report  \n",
      "0         NaN   NaN            NaN            NaN  \n",
      "1         0.0   0.0            0.0            0.0  \n",
      "2         0.0   0.0            0.0            0.0  \n",
      "3         0.0   0.0            0.0            0.0  \n",
      "4         0.0   0.0            0.0            0.0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the file paths to your CSV files\n",
    "messages_filepath = 'messages.csv'\n",
    "categories_filepath = 'categories.csv'\n",
    "\n",
    "# Call the load_data function\n",
    "df, X = load_data(messages_filepath, categories_filepath)\n",
    "\n",
    "# Print the contents of df and X\n",
    "print(\"DataFrame (df):\")\n",
    "print(df.head())  # Print the first few rows of the DataFrame\n",
    "\n",
    "print(\"\\nFeatures (X):\")\n",
    "print(X.head())  # Print the first few rows of the 'message' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_filepath = 'messages.csv'  # Pfad zur Nachrichten-CSV-Datei\n",
    "categories_filepath = 'categories.csv'  # Pfad zur Kategorien-CSV-Datei\n",
    "\n",
    "X, y = load_data(messages_filepath, categories_filepath)  # Daten laden\n",
    "model = build_model()  # Modell erstellen\n",
    "model = train(X, y, model)  # Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
